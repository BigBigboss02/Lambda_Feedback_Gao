import os
import argparse
import json
from tqdm import tqdm
from transformers import AutoTokenizer
from transformers import LlamaForCausalLM
from transformers import LlamaTokenizer, WhisperFeatureExtractor
from transformers import GenerationConfig
from src.modeling_blsp import BlspModel
from src.speech_text_paired_dataset import get_waveform
from peft import LoraConfig, get_peft_model
from peft import PeftModel, PeftConfig
from transformers.deepspeed import is_deepspeed_zero3_enabled

generation_config = GenerationConfig(
    max_new_tokens=512,
    min_new_tokens=1,
    do_sample=False,
    temperature=0.1,
    top_p=0.75,
    num_beams=1,
    num_return_sequences=1,
)


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--input_file", type=str, default="/data/jbn/formatted_data/datasets/testSIMS.jsonl",
        help="Path to the input file", 
    )
    parser.add_argument(
        "--output_file", type=str, default="/data/jbn/formatted_data/stage2/testSIMS_withfreeze.jsonl",
        help="Path to the output file", 
    )
    parser.add_argument(
        #这里放第二部练的adapter
        #最优组合 /data/jbn/pretrained_models/vicuna_speech
        #
        "--blsp_model", type=str, default="/data/jbn/pretrained_models/vicuna_freeze_stage2",
        help="Path to the blsp model", 
    )
    ### args for generation
    parser.add_argument(
        "--max_new_tokens", type=int, default=1024,
        help="max new tokens for generation"
    )
    parser.add_argument(
        "--min_new_tokens", type=int, default=1,
        help="min new tokens for generation"
    )
    parser.add_argument(
        "--do_sample", action="store_true",
        help="whether do sample. For ST task, we will use greedy search to ensure stable output"
    )
    parser.add_argument(
        "--temperature", type=float, default=0.1,
        help="temperature for generation"
    )
    parser.add_argument(
        "--top_p", type=float, default=0.75,
        help="top_p for generation"
    )
    parser.add_argument(
        #这里放第一步练的lora
        #最优组合 /data/jbn/pretrained_models/vicuna_test
        #
        "--peft_model_id", type=str, default="/data/jbn/pretrained_models/vicuna_freeze_stage2/llm",
        help="checkpoint of peft"
    )

    parser.add_argument(
        #这里放基座模型
        #最优组合 /data/jbn/pretrained_models/vicuna
        #
        "--llama_model", type=str, default="/data/jbn/pretrained_models/vicuna",
        help="checkpoint of peft"
    )
    args = parser.parse_args()
    #更新一下tokenizer
    # tokenizer = AutoTokenizer.from_pretrained("/root/.cache/modelscope/hub/ZhipuAI/chatglm3-6b-base", trust_remote_code=True)

    tokenizer = LlamaTokenizer.from_pretrained(args.blsp_model)
    # tokenizer = LlamaTokenizer.from_pretrained(args.llama_model)

    extractor = WhisperFeatureExtractor.from_pretrained(args.blsp_model)
    model = BlspModel.from_pretrained(args.blsp_model)
    
    # model = 
    generation_config.update(
        **{
            "max_new_tokens": args.max_new_tokens,
            "min_new_tokens": args.min_new_tokens,
            "do_sample": args.do_sample,
            "temperature": args.temperature,
            "top_p": args.top_p,
            "pad_token_id": tokenizer.pad_token_id,
            "bos_token_id": tokenizer.bos_token_id,
            "eos_token_id": tokenizer.eos_token_id
        }
    )
    llama_model = LlamaForCausalLM.from_pretrained(args.llama_model, _fast_init=not is_deepspeed_zero3_enabled())
    model.llama_model =  PeftModel.from_pretrained(llama_model,args.peft_model_id)
    model = model.cuda()
    model.eval()
    with open(args.input_file, "r") as fin, open(args.output_file, "w") as fout:
        for line in tqdm(fin):
            data = json.loads(line.strip())
            # /data/jbn/formatted_data/stage2/8kto24k_train_stage2.jsonl
            # inference = data.get("instruction", args.instruction)
            # instruction = "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n ### Instruction:\nYou are an ASR transcript selector and a sequence tagging expert. Your task is to modify the sentences generated by an ASR model to get the most likely correct sentence, and then perform following steps . 1.We can use [] to label PER entities2.We can use <> to label ORG entities3.We can use () to label LOC entities\n\n### Input:\n"

#             instruction=( "As an expert in sentiment analysis and voice recognition,"
#             " your task involves processing audio speech. These audio speech contain sentences with positive, negative emotions. "
#             "To complete the task, firstly, transcribe the audio into Chinese text, then identify and mark the emotive words. The key criterion for emotion-expressive words is that their replacement alters the emotional tone of the sentence. "
#             "Importantly, these words are generally not names, places, dates, or pronouns.  "
#             "Focus on (1) understanding the sentence's emotion without personal bias,"
#            " and (2) note that marked words typically do not include the targets of the emotion, such as pronouns or names, as their replacement would not affect the overall sentiment."
# " Attention: Please mark the sentence by encapsulating phrases with negative emotion within  {} to denote their beginning and end. Similarly, encapsulate phrases with positive emotion within [] to clearly indicate their start and finish."
#             )
            # full_prompt = f"""Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n ### Instruction:\n{instruction}\n### Input:\n{inference}\n### Speech features:\n"""
            # full_prompt = f"""Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n ### Instruction:\n{instruction}\n### Input:\n"""
            # full_prompt = f"""{instruction}\n### Input:\n"""
            

            # prompt ="As a sentiment analysis and voice recognition expert, your task is to transcribe audio speech into Chinese text and mark emotive words, focusing on those whose substitution changes the sentence's emotional tone. Avoid personal bias and note that names, places, dates, and pronouns usually don't convey emotion directly. Mark negative phrases with [], and positive ones with {}."
            # full_prompt = f"""Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n ### Instruction:{prompt}\n### speech features:\n"""
            prompt = ( "As an expert in sentiment analysis and voice recognition,"
            " your task is to correct the output to form the most accurate sentence based on multiple potential transcription results and speech features outputted by the ASR model. "
            "Once get the accurate sentence,  then you need to identify and mark the emotive words. The key criterion for emotion-expressive words is that their replacement alters the emotional tone of the sentence. "
            "Focus on identifying words that are not names, places, dates, or pronouns, as marked words generally exclude emotion targets like pronouns or names which don't affect overall sentiment, and mark negative phrases with [] and positive ones with {}."
            )
            full_prompt = f"""Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n ### Instruction:{prompt}\n###Input:\n{data['inference']}\n### Speech Feautres:\n"""

            input_ids = tokenizer(f"###[Human]:{full_prompt}", return_tensors="pt").input_ids.cuda()

            audio = data["audio_path"]#.get("audio_file_path", None)# 原来是audio
            # audio = None
            speech_values, speech_attention_mask = None, None
            if audio is not None:
                speech = get_waveform(audio, output_sample_rate=extractor.sampling_rate)
                speech_inputs = extractor(
                    speech,
                    sampling_rate=extractor.sampling_rate,
                    return_attention_mask=True,
                    return_tensors="pt"
                )
                speech_values = speech_inputs.input_features.cuda()
                speech_attention_mask = speech_inputs.attention_mask.cuda()
            else:
                print(audio)
            suffix_input_ids = tokenizer("\n\n\n###[Assistant]:", return_tensors="pt").input_ids[:,1:].cuda()
            reference = data['text']

            output = model.generate(
                input_ids=input_ids,
                suffix_input_ids=suffix_input_ids,
                speech_values=speech_values,
                speech_attention_mask=speech_attention_mask,
                generation_config=generation_config,
            )
            response = tokenizer.decode(output[0], skip_special_tokens=True)

            json_string = json.dumps(
                {
                    "response": response,
                    "reference": reference
                },
                ensure_ascii=False
            )
            fout.write(json_string + "\n")
            

if __name__ == "__main__":
    main()