# Lambda_Feedback_Gao

This is the repository of Gao Zhuangfei's Final Year Project.

## ğŸ“¦ Setup
1. Install all dependencies from `requirements_vs3.txt`.
2. Set up your login_configs.env following the format of pseudo_login_configs.env
3. Download necessary base models from Huggingface (see External Resources)
4. Set the folder paths in the funtion you are using to your local folder location 
5. Local NVDIA GPU or Apple silican might be needed, situation depending on the function you are using


## ğŸ“ Repository Overview

- `functions/`: All executable modules including Python scripts, PostgreSQL scripts, and LoRA training components.
- `test_results/`: Outputs from evaluation runs on various large language models (LLMs), including both baseline and LoRA-augmented tests.
- `non_functions/`: Supplementary resources such as notes, version control records, and environment setup scripts.
- `shortTextAnswer/`: Forked branch from the Lambda Feedback repo â€” refer to its internal `README` for app-specific instructions.

## ğŸ”— External Resources
- Huggingface base models to download and use:  
  ğŸ‘‰ [LLaMA-3.2-1B](https://huggingface.co/meta-llama/Llama-3.2-1B)  
  ğŸ‘‰ [LLaMA-3.1-8B](https://huggingface.co/Bigbigboss02/llama3.1-8B)  
  ğŸ‘‰ [BERT Large Uncased](https://huggingface.co/Bigbigboss02/bert-large-uncased)  
- HuggingFace datasets and models for Colab integration:  
  ğŸ‘‰ [https://huggingface.co/Bigbigboss02](https://huggingface.co/Bigbigboss02)

---

## ğŸ“‚ Project Folder Structure
<pre lang="markdown">
```
Lambda_Feedback_Gao/

â”œâ”€â”€ non_functions/                    # Non-experiment resources
â”‚   â”œâ”€â”€ version_control/
â”‚   â”œâ”€â”€ cuda_env_test/
â”‚   â””â”€â”€ notes_and_literatures/

â”œâ”€â”€ functions/                        # Experiment-related code and modules
â”‚   â”œâ”€â”€ google_colab_scripts/        # High value â€” LoRA training notebook
â”‚   â”œâ”€â”€ postgreSQL_scripts/          # Low value â€” initial SQL scripts
â”‚   â”‚   â””â”€â”€ tests/
â”‚   â”œâ”€â”€ LoRA_finetuning_scripts/     # High value â€” LoRA training/testing
â”‚   â”‚   â”œâ”€â”€ calling_functions/       # High value â€” LoRA model interface
â”‚   â”‚   â”œâ”€â”€ adaptors/                # High/Moderate value â€” trained LoRA weights
â”‚   â”‚   â”‚   â”œâ”€â”€ tuned_Llama321B_adaptors/
â”‚   â”‚   â”‚   â”œâ”€â”€ tuned_BeRT_adaptors/
â”‚   â”‚   â”‚   â”œâ”€â”€ tuned_Llama321B_balanced_dataset/
â”‚   â”‚   â”‚   â””â”€â”€ selected_Llama321B_adaptors/
â”‚   â”‚   â”œâ”€â”€ testing_functions/       # Moderate value
â”‚   â”‚   â””â”€â”€ data/
â”‚   â”‚       â””â”€â”€ waste_data/          # Low value
â”‚   â””â”€â”€ Prompt_Engineering_scripts/  # Moderate value â€” prompt logic + API
â”‚       â”œâ”€â”€ Archive/                 # Low value â€” archived code
â”‚       â”œâ”€â”€ __pycache__/
â”‚       â”œâ”€â”€ Initial_experiments/
â”‚       â”‚   â”œâ”€â”€ tools/
â”‚       â”‚   â”‚   â””â”€â”€ __pycache__/
â”‚       â”‚   â”œâ”€â”€ tests/
â”‚       â”‚   â””â”€â”€ structured_prompts/
â”‚       â”‚       â”œâ”€â”€ LongChain/
â”‚       â”‚       â””â”€â”€ confusion_matrix/
â”‚       â””â”€â”€ Severless_API_calls/     # Moderate value â€” GPT-4o-mini endpoint calls

â”œâ”€â”€ test_results/                    # High value â€” model test results
â”‚   â”œâ”€â”€ Larger_LLM_tests/
â”‚   â”‚   â”œâ”€â”€ one_to_many_Cross_Platform_Comparison/
â”‚   â”‚   â”‚   â”œâ”€â”€ Llama3_3B_test/
â”‚   â”‚   â”‚   â”œâ”€â”€ GPT-4o_mini_8B_test/
â”‚   â”‚   â”‚   â””â”€â”€ Llama3_1B_test/
â”‚   â”‚   â”œâ”€â”€ one_to_one_Cross_Platform_Comparison/
â”‚   â”‚   â”‚   â””â”€â”€ [various GPT-4o-mini and Llama3.2-1B result folders]
â”‚   â”‚   â””â”€â”€ initial_model_tests/
â”‚   â”‚       â”œâ”€â”€ week15_experiments/
â”‚   â”‚       â””â”€â”€ week16_experiments/
â”‚   â”œâ”€â”€ Wasted_Data/
â”‚   â””â”€â”€ Smaller_LLM_tests/
â”‚       â”œâ”€â”€ LoRa_controlled_variable_tests/
â”‚       â”‚   â”œâ”€â”€ BeRT_large/                             # BeRT testing results
â”‚       â”‚   â”‚   â””â”€â”€ confusion_matrices/
â”‚       â”‚   â””â”€â”€ Llama3-1B_balanced/                     # Trained on Balanced training dataset
â”‚       â”‚       â”œâ”€â”€ instructive_prompt/                 # Named by prompt type used at testing
â”‚       â”‚       â”‚   â”‚                                   # Csv file named prompt/arg type used at fine-tuning
â”‚       â”‚       â”‚   â””â”€â”€ parsed_results/                 # Parsed results
â”‚       â”‚       â”‚       â””â”€â”€ confusion_matrices/         # Confusion matrices ploted
â”‚       â”‚       â””â”€â”€ instructive_examples_prompt/
â”‚       â”‚           â””â”€â”€ parsed_results/
â”‚       â”‚               â””â”€â”€ confusion_matrices/
â”‚       â”‚   â””â”€â”€ Llama3-1B_main/                         # Trained on Main training dataset
â”‚       â”‚       â”œâ”€â”€ balanced_training_material_prompt/
â”‚       â”‚       â”‚   â”œâ”€â”€ instructive_prompt/
â”‚       â”‚       â”‚   â”‚   â””â”€â”€ parsed_results/
â”‚       â”‚       â”‚   â”‚       â””â”€â”€ confusion_matrices/
â”‚       â”‚       â”‚   â””â”€â”€ instructive_examples_prompt/
â”‚       â”‚       â”‚       â””â”€â”€ parsed_results/
â”‚       â”‚       â”‚           â””â”€â”€ confusion_matrices/
â”‚       â”‚       â”œâ”€â”€ main_training_material_prompt/
â”‚       â”‚       â”‚   â”œâ”€â”€ instructive_prompt/
â”‚       â”‚       â”‚   â”‚   â””â”€â”€ parsed_results/
â”‚       â”‚       â”‚   â”‚       â””â”€â”€ confusion_matrices/
â”‚       â”‚       â”‚   â”œâ”€â”€ examples_prompt/
â”‚       â”‚       â”‚   â”‚   â””â”€â”€ parsed_results/
â”‚       â”‚       â”‚   â”‚       â””â”€â”€ confusion_matrices/
â”‚       â”‚       â”‚   â”œâ”€â”€ instructive_examples_prompt/
â”‚       â”‚       â”‚   â”‚   â””â”€â”€ parsed_results/
â”‚       â”‚       â”‚   â”‚       â””â”€â”€ confusion_matrices/
â”‚       â”‚       â”‚   â””â”€â”€ null_prompt/
â”‚       â”‚       â”‚       â””â”€â”€ parsed_results/
â”‚       â”‚       â”‚           â””â”€â”€ confusion_matrices/
â”‚       â”‚       â””â”€â”€ new_testing_material_prompt/
â”‚       â”‚           â”œâ”€â”€ instructive_prompt/
â”‚       â”‚           â”‚   â””â”€â”€ parsed_results/
â”‚       â”‚           â”‚       â””â”€â”€ confusion_matrices/
â”‚       â”‚           â””â”€â”€ instructive_examples_prompt/
â”‚       â”‚               â””â”€â”€ parsed_results/
â”‚       â”‚                   â””â”€â”€ confusion_matrices/
â”‚       â”œâ”€â”€ Base_controlled_variable_tests/
â”‚       â”‚   â”œâ”€â”€ BeRT_large/
â”‚       â”‚   â”‚   â””â”€â”€ confusion_matrices/
â”‚       â”‚   â””â”€â”€ llama32_1b/
â”‚       â”‚       â””â”€â”€ parsed_results/
â”‚       â”‚           â””â”€â”€ confusion_matrices/
â”‚       â”œâ”€â”€ Llama3_1B_initial_test/
â”‚       â””â”€â”€ initial_finetuned_model_test/ # Low value, initial attempts

```
</pre>

